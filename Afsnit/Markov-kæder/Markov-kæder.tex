
%Markov-kæder har en bestemt \textbf{\textit{afhængighedsstruktur}}. 
Det der karakteriserer en markov-kæde er, at sandsynligheden for en tilfældig variabel til indekset $t+1$ kun afhænger af udfaldet af den tilfældige variabel til indekset $t$. Altså har Markov-kæder en bestemt afhængighedsstruktur. Dette er formuleret i følgende definition.

\begin{minipage}\textwidth
\begin{defn}\textbf{Markov egenskaben} %Ny definition
\newline
Lad $\bm{X}=(X_0, X_1, \dots)$ være en sekvens af diskrete tilfældige variabler, som antager værdier i et tilstandsrum, $S$ og lad $i_0, i_1, \ldots, i_{t}, j\in S$ være tilstande. 
Sekvensen $\bm{X}$ er en Markov-kæde, hvis den opfylder \textit{Markov egenskaben}
\begin{align}\label{eq:markov_egenskaben}
    P(X_{t+1} = j | X_0 = i_0, \dots, X_{t-1} = i_{t-1}, X_t = i_t) =  P(X_{t+1} = j | X_t = i_t)
\end{align}
for alle $t\geq 0$ samt $i_0, i_1, \cdots i_{t}, j\in S$.
Markov-kæden kaldes homogen, hvis den betingede sandsynlighed, $P(X_{t+1}=j|X_t=i)$ ikke afhænger af $t$ for alle $i,j\in S$.
\end{defn}
\end{minipage}

I ovenstående definition siges $X_t$ at være \textit{tilstanden} af kæden til indeks $t$. Tilstandsrummet, $S$, kan både være endeligt og tællelig uendeligt. I de følgende afsnit antages alle Markov-kæder at være homogene.


\section{Overgangssandsynlighed}
Sandsynligheden for at gå fra en tilstand til en anden  kaldes for \textit{overgangssandsynligheden} og præsenteres i følgende definition.

\begin{minipage}\textwidth
\begin{defn}\textbf{Overgangssandsynligheden} \label{def:overgangssandsynlighed} %Ny definition
\newline
Lad $\bm X = (X_0, X_1, \ldots)$ være en Markov-kæde med tilstandsrummet $S$ og lad $i,j\in S$ være tilstande. Overgangssandsynligheden for $\bm X$ er givet ved $$p_{ij}=P(X_{t+1}=j|X_t=i).$$
\end{defn}
\end{minipage}

Overgangssandsynligheden, $p_{ij}$, beskriver dermed sandsynligheden for at gå fra tilstanden $i$ til tilstanden $j$. Det følger af \autoref{prop:frekvensfunktion}, at
%
\begin{align*}
    &p_{ij}\geq 0 \text{ for alle } i,j\in S \text{ og}\\
    &\sum_{j\in S} p_{ij}=1 \text{ for alle } i\in S.
\end{align*}


Såfremt ovenstående er opfyldt, er det muligt at definere \textit{overgangsmatricen} som følgende

\begin{minipage}\textwidth
\begin{defn}\textbf{Overgangsmatrice}\label{def:ovegangsmatrice} %Ny definition
\newline
Lad $\bm X$ være en Markov-kæde med tilstandsrummet $S$ og $i,j \in S$ være tilstande. Lad derudover $p_{ij}$ være overgangssandsynligheden. Så kaldes matricen defineret ved
\begin{align*}
    \P = \left[p_{ij}\right]
\end{align*}
overgangsmatricen for $\bm X$, hvor $p_{ij}$ er den $ij$'te indgang i $\mathcal{P}$.
\end{defn}
\end{minipage}

I følgende eksempel, \autoref{eks:overgangsmatrice}, visualiseres en Markov-kæde som en graf, også kaldet en \textit{overgangsgraf}. Herudover opstilles en overgangsmatrice.

\begin{eks}\textbf{} \label{eks:overgangsmatrice}%Nyt eksempel
\newline
Lad $\bm X = (X_0, X_1, \ldots)$ være en Markov-kæde med tilstandsrummet $S = \{1, 2, 3\}$. Lad derudover overgangssandsynlighederne mellem hver tilstand være givet ved
\begin{align*}
    p_{12} = 
    p_{13} = 
    p_{22} = 
    p_{23} =
    p_{31} = 
    p_{32} = \frac{1}{2},
\end{align*}
og de resterende overgangssandsynligheder være lig nul. Dermed ser overgangsgrafen ud på følgende måde
\begin{center}
	\begin{tikzpicture}[->, >=stealth', auto, semithick, node distance=3cm]
	\tikzstyle{every state}=[fill=white,draw=black,thick,text=black,scale=1]
	\node[state]    (1)                     {$1$};
	\node[]         (5)[right of=1]   {};
	\node[state]    (2)[right of=5]   {$2$};
	\node[]         (4)[below of=1]   {};
	\node[state]    (3)[right of =4]    {$3$};
	\path
	(1) edge[bend left,above]	node{$1/2$}	(2)
	(1) edge[bend left,right]	node{$1/2$}	(3)
	(2) edge[loop above]		node{$1/2$}	(2)
	(2) edge[bend left,right]	node{$1/2$}	(3)
	(3) edge[bend left,left]		node{$1/2$}	(1)
	(3) edge[bend left,left]		node{$1/2$}	(2);
	\end{tikzpicture}
\end{center}
I dette tilfælde er overgangsmatricen være givet ved
\begin{align*}
    \mathcal{P}=
\begin{bmatrix}
0 & \frac{1}{2} & \frac{1}{2}\\
0 & \frac{1}{2} & \frac{1}{2}\\
\frac{1}{2} & \frac{1}{2} & 0
\end{bmatrix},
\end{align*}
hvor $p_{ij}$ er den $ij$'te indgang. 
\end{eks}

Følgende sætning er en betingelse for, at en given sekvens af diskrete tilfældige variabler er en Markov-kæde.

\begin{minipage}\textwidth
\begin{thmx} \textbf{Markov betingelsen} \label{sæt:markov_tingtang}%Ny sætning
\newline 
Lad $\bm X=(X_0, X_1,\ldots)$ være en sekvens af diskrete tilfældige variabler med tilstandsrum, $S$ og lad $i_0, i_1, \ldots, i_t \in S$ være tilstande. Lad derudover $p_{ij}$ være overgangssandsynligheden. Så er $\bm X$ en Markov-kæde med overgangsmatricen, $\mathcal{P}$, hvis og kun hvis
\begin{align}
    P(X_0=i_0,X_1=i_1,\dots, X_t=i_t)=P(X_0=i_0)p_{i_0i_1}\dots p_{i_{t-1}i_t}\label{markov-kæde-hovedsætning}
\end{align}
for alle $t\geq0$ og $i_0,i_1,\dots,i_t\in S$. Begyndelsessandsynligheden er givet ved $P(X_0 = i_0)$.
\end{thmx}
\end{minipage}
\begin{bev} \textbf{}
\newline
Lad $\bm X = (X_0, X_1, \ldots)$ være en sekvens af diskrete tilfældige variabler og $i_0, i_1, \ldots, i_t \in S$ være tilstande. \\
Betegn $A_k = \{X_k = i_k\}$ og $\bm A_t=\bigcap_{k=0}^t A_k$. Hvis $P(\bm A_t)=0$, er beviset trivielt, og derfor antages det, at $P(\bm A_t)>0$ for alle $t$. 
Med ovenstående notation er \eqref{markov-kæde-hovedsætning} ækvivalent med følgende
\begin{align}\label{eq:omskrivning}
    %P(\{X_0=i_0\}\cap \{X_1=i_1\}\cap\dots\cap \{X_n=i_n\})=
    P(\bm A_t)=P(A_0)p_{i_0i_1}\cdots p_{i_{t-1}i_t}.
\end{align}
Dermed kan \eqref{markov-kæde-hovedsætning} bevises, ved at vise \eqref{eq:omskrivning}.

Først bevises det, at såfremt $\bm X$ er en Markov-kæde, så gælder \eqref{eq:omskrivning}. Dette gøres ved induktion.
Antag derfor, at $\bm X$ er en Markov-kæde med begyndelsesfordelingen $P(A_0)$ og overgangsmatricen $\P$. 

For $t=0$ gælder det, at $P(\bm A_0) = P(A_0) = P(X_0=i_0)$, og derved er induktionsstarten vist. Antag, at \eqref{eq:omskrivning} gælder for $t$. Det skal vises, at dette medfører, at \eqref{eq:omskrivning} er opfyldt for $t+1$. Af \autoref{def:betinget_sandsynlighed}, \eqref{eq:markov_egenskaben} og \autoref{def:overgangssandsynlighed} fås
%
\begin{align*}
     P(\bm A_{t+1}) = P(A_{t+1}\cap \bm A_{t})= P(\bm A_{t})P(A_{t+1}| A_t)=P(A_0)p_{i_0i_1}\dots p_{i_{t}i_{t+1}}
\end{align*}
Altså er induktionsskridtet vist. Det gælder dermed, at hvis $\bm X$ er en Markov-kæde, så er \eqref{eq:omskrivning} sand.  

Herefter vises det, at hvis \eqref{eq:omskrivning} gælder, så er Markov egenskaben, \eqref{eq:markov_egenskaben}, opfyldt. Antag derfor, at \eqref{eq:omskrivning} gælder for alle $t$ samt alle sekvenser $(i_m)$. Når $t = 0$, fås $P(\bm A_0)=P(A_0)$. Det følger af \autoref{def:betinget_sandsynlighed}, at
\begin{align*}
    P(A_{t+1}|\bm A_t)&=\frac{P(\bm A_{t+1})}{P(\bm A_t)}\\
    &= \frac{P(A_0)p_{i_0i_1}\cdots p_{i_{t-1}i_t}}{P(A_0)p_{i_0i_1}\cdots p_{i_{t-1}i_t}}\cdot p_{i_{t}i_{t+1}}
    \\
    &=p_{i_ni_{t+1}} = P(A_{t+1} | A_t).
\end{align*}
Altså er Markov egenskaben opfyldt, og $\bm X$ er en Markov-kæde med overgangsmatricen, $\P$. Dermed er \autoref{sæt:markov_tingtang} bevist. 

\end{bev}


\section{Tidsdynamik af Markov-kæder}
Hvis overgangssandsynligheden $p_{ij}$ er kendt, er det muligt at bestemme fremtidige overgangssandsynligheder. Overgangsmatricen består af $p_{ij}$, som er \textit{første trins overgangssandsynligheder}, hvor man generelt kan skrive \textit{t'te trins overgangssandsynligheder} som 
\begin{align}\label{eq:nte-trinsovergangssandsynligheden}
    p_{ij}^{(t)} = P ( X_t = j | X_0 = i).
\end{align}
Sandsynligheden for at gå fra tilstanden $i$ til $j$ efter $t$ trin er dermed givet ved $p_{ij}^{(t)}$. Ud fra $p_{ij}^{(t)}$ defineres $t$'te trins overgangsmatricen som $\P^{(t)}=\left[p_{ij}^{(t)}\right]$. Indgangene i matricen opfylder følgende

\begin{minipage}\textwidth
\begin{thmx} \textbf{Chapman-Kolmogorov ligningen}\label{sæt:chapman-kolmogrov} %Ny sætning
\newline
Lad $\bm X$ være en Markov-kæde med tilstandsrummet $S$ og lad $i,j, k \in S$ være tilstande. Lad derudover $p_{ij}^{(t)}$ være $t$'te trins overgangssandsynligheden. Da gælder det, at
\begin{align*}
    p_{ij}^{(t+m)} = \sum_{k \in S} p_{ik}^{(t)}p_{kj}^{(m)}
\end{align*}
for alle $t, m \geq 0$ og alle $i,j \in S$. 
\end{thmx}
\end{minipage}
\begin{bev} \textbf{} %Nyt bevis
\newline
Lad $\bm X$ være en Markov-kæde med tilstandsrummet $S$. Lad derudover $p_{ij}^{(t)}$ være $t$'te trins overgangssandsynligheden og lad $i,j, k \in S$ være tilstande. Eftersom Markov-kæden er homogen, og ved brug af loven om total sandsynlighed, \autoref{sæt:loven_om_total_sandsynlighed}, gælder det, at
\begin{align*}
    p_{ij}^{(t+m)} &= P(X_{t+m} = j | X_0 = i) = \sum_{k \in S} P (X_{t+m} = j | X_t = k, X_0 = i)P(X_t=k|X_0=i).
    \intertext{Af Markov egenskaben, \eqref{eq:markov_egenskaben}, fås dermed, at}
    p_{ij}^{(t+m)} &= \sum_{k \in S} P (X_{t+m} = j | X_t = k)P(X_t=k|X_0=i) \\ 
    &= \sum_{k \in S} p_{ik}^{(t)}p_{kj}^{(m)}.
\end{align*}
Dermed er \autoref{sæt:chapman-kolmogrov} bevist.
\end{bev}
Bemærk, at det følger af \autoref{sæt:chapman-kolmogrov}, at
\begin{align} \label{eq:chapman-kolmogorov}
    \P^{(t+m)} = \P^{(t)}\P^{(m)}
\end{align}
hvor $\P^{(t)}$ er $t$'te trins overgangsmatricen.
%hvor $\P^{(t)}$ og $\P^{(m)}$ er henholdsvis $t$'te trins og $m$'te trins overgangsmatricen

Hvis overgangsmatricen, $\P$, er kendt, er det muligt at bestemme $\P^{(t)}$ uden at skulle beregne $p_{ij}^{(t)}$.
Dette præsenteres i følgende sætning.

\begin{minipage}\textwidth
\begin{thmx}\textbf{Overgangsmatricen ved $\bm t$'te trin} \label{sæt:P(n)Pn} \\
Lad $\bm X$ være en Markov-kæde med tilstandsrummet, $S$. Lad $\P^{(t)}$ være $t$'te trins overgangsmatricen. Så gælder det, at
    \begin{align*}
        \P^{(t)} = \P^t.
    \end{align*}
\end{thmx}
\end{minipage}

\begin{bev}\textbf{}\\
Lad $\bm X = (X_0, X_1, \ldots)$ være en Markov-kæde med tilstandsrummet $S$ og $i,j\in S$ være tilstande. Lad $\P^{(t)}$ være $t$'te trins overgangsmatricen. For $t=1$ gælder det ud fra \autoref{def:overgangssandsynlighed}, at
\begin{align*}
    p_{ij}^{(1)} = P(X_1=j|X_0=i) = p_{ij}.
\end{align*}
Af \autoref{def:ovegangsmatrice} fås $\P^{(1)} = \left[p_{ij}^{(1)}\right] = \left[p_{ij}\right]= \P$. Ud fra \eqref{eq:chapman-kolmogorov} gælder det, at
\begin{align*}
    \P^{(t)}= \P^{(t-1)}\P^{(1)}=\prod_{k=1}^{t} \P^{(1)} = %P^{(1)}\cdots P^{(1)} = P^{1}\cdots P^1 = 
    \prod_{k=1}^{t} \P = \P^{t}.
\end{align*}
Dermed er \autoref{sæt:P(n)Pn} bevist.
\end{bev}


\section{Klassifikation af tilstande}

I dette afsnit præsenteres sammenhængen mellem tilstandsrummet, $S$, og overgangsmatricen, $\P$. Hvis overgangssandsynligheden mellem to tilstande er positiv, er det muligt at gå fra den ene tilstand til den anden. Dette præsenteres i følgende definition

\begin{minipage}\textwidth
\begin{defn}\textbf{Tilgængelighed} \label{def:tilgængelighed} %Ny definition
\newline
Lad $\bm X$ være en Markov-kæde med tilstandsrummet, $S$. Lad derudover $p_{ij}^{(t)}$ være $t$'te trins overgangssandsynligheden, og lad $i,j \in S$ være tilstande. Hvis der eksisterer et $t\in T$, hvorom der gælder, at $p_{ij}^{(t)}>0$, siges tilstanden, $j$, at være \textit{tilgængelig} fra tilstanden $i$. Dette noteres, $i\to j$. Hvis $i\to j$ og $j\to i$, siges det, at $i$ og $j$ \textit{kommunikerer}, hvilket noteres $i\leftrightarrow j$.
\end{defn}
\end{minipage}

Hvis der for en delmængde af $S$ gælder, at alle tilstande kommunikerer, kaldes delmængden en \textit{kommunikerende klasse}. Det er muligt, at alle tilstande i $S$ kommunikerer, hvilket fører til følgende definition 

\begin{minipage}\textwidth
\begin{defn}\label{def:ureducerbar} \textbf{Ureducerbar} %Ny definition
\newline
Lad $\bm X$ være en Markov-kæde med tilstandsrummet $S$. $\bm X$ siges at være \textit{ureducerbar}, hvis der for alle tilstande, $i,j \in S$ gælder, at $i \leftrightarrow j$.
\end{defn}
\end{minipage}

Ud over at det er muligt at bestemme om tilstande kommunikerer, kan det undersøges, hvorvidt det er muligt at komme tilbage til en tilstand. Heraf defineres følgende



\begin{minipage}\textwidth
\begin{defn}\textbf{Tilbagevendende og forbigående tilstande} \label{def:tau}%Ny definition
\newline
Lad $\bm X = (X_0, X_1, \dots)$ være en Markov-kæde med tilstandsrummet $S$. Lad derudover $i\in S$ være en tilstand og
\begin{align*}
    \tau_i=\min\{t\geq1:X_t=i\}.
\end{align*}
Hvis $P(\tau_i<\infty|X_0=i)=1$, siges tilstanden, $i$, at være \textit{tilbagevendende}. Hvis den ikke er tilbagevendende, siges den at være \textit{forbigående}.
\end{defn}
\end{minipage}

Altså er $\tau_i$ antallet af trin før kæden besøger $i$.

Hvis en tilstand er tilbagevendende, vil Markov-kæden med sikkerhed vende tilbage til tilstanden. Hvis tilstanden derimod er forbigående, vil der være en positiv sandsynlighed for, at Markov-kæden ikke vender tilbage til tilstanden. 

For at bestemme om en tilstand er tilbagevendende kan  følgende resultat anvendes.

\begin{minipage}\textwidth
\begin{thmx}\label{tilbagevendende} \textbf{Tilbagevendende tilstand} %Ny proposition
\newline
Lad $\bm X$ være en Markov-kæde med tilstandsrummet, $S$ og lad $i,j \in S$ være tilstande. Lad derudover $p_{ij}^{(t)}$ være $t$'te trins overgangssandsynligheden. Tilstanden $i$, er tilbagevendende, hvis
\begin{align*}
 \sum_{t=1}^\infty p_{ii}^{(t)}=\infty.
\end{align*}
\end{thmx}
\end{minipage}

\begin{bev} \textbf{} %Nyt bevis
\newline
Lad $\bm X = (X_0, X_1, \dots)$ være en Markov-kæde med tilstandsrummet $S$. Lad derudover $p_{ij}^{(t)}$ være $t$'te-trins overgangssandsynligheden og $i,j \in S$ være tilstande.

I dette bevis defineres indikatorfunktionen for alle $t \in \N$ som følgende
\begin{align*}
    I_t = 
  \begin{cases}
    1       & \quad \text{hvis } X_t = i\\
    0  & \quad \text{hvis } X_t \neq i
  \end{cases}
\end{align*}
Ud fra indikatorfunktionen, $I_t$, defineres antallet af gange Markov-kæden vender tilbage til tilstanden $i$, som 
\begin{align*}
    A = \sum_{t = 1}^{\infty} I_t.
\end{align*}
Det antages for modstrid, at Markov-kæden er forbigående, altså at $E\left[A | X_0 = i\right] < \infty$.

Det gælder, at
\begin{align}
    \infty > E\left[A | X_0 = i\right] &= \sum_{t=1}^{\infty} E\left[I_t | X_0 = i\right]. \label{lorte_eq}
    \intertext{Fra \autoref{def:betinget_forventet_værdi_af_diskrete_tilfældige_variabler} gælder det, at}
\sum_{t=1}^{\infty} E\left[I_t | X_0 = i\right] &= \sum_{t=1}^{\infty} \sum_{x \in \text{Range}(I_t)} xP\left(X_t=x | X_0 = i\right) = \sum_{t=1}^{\infty} P\left(X_t=i | X_0 = i\right). \nonumber
    \intertext{Af \eqref{eq:nte-trinsovergangssandsynligheden} følger det da, at}
    \sum_{t=1}^{\infty} P\left(X_t=i | X_0 = i \right) &= \sum_{t=1}^{\infty} p_{ii}^{(t)}.\nonumber
\end{align} 
Dette er en modstrid, da det er antages, at $\displaystyle \sum_{t=1}^{\infty} p_{ii}^{(t)} = \infty$. Altså må det gælde, at Markov-kæden er tilbagevendende, og dermed er \autoref{tilbagevendende} bevist. 

Det skal påpeges, at \eqref{lorte_eq} følger af Fubini-Tonelli's sætning, som ikke vil gennemgås i dette projekt. 
\end{bev}

For en ureducerbar Markov-kæde gælder følgende korollar

\begin{minipage}\textwidth
\begin{kor} \textbf{} \label{kor:enten_forbigå_eller_tilbagevend}%Nyt k
\newline
Lad $\bm X$ være en ureducerbar Markov-kæde. Så gælder det, at alle tilstande enten er forbigående eller tilbagevendende.
\end{kor}
\end{minipage}

\begin{bev} \textbf{} %Nyt bevis
\newline
Lad $\bm X$ være en Markovkæde med tilstandsrummet $S$. Lad derudover $p_{ij}^{(t)}$ være $t$'te-trins overgangssandsynligheden og $i,j \in S$ være tilstande.

Det bevises, at hvis én tilstand er tilbagevendende, medfører det, at alle tilstande er tilbagevendende. Altså skal alle tilstande enten være tilbagevendende eller forbigående. Fra \autoref{tilbagevendende} er det derved tilstrækkeligt at bevise, at %
\begin{align*}
    \sum_{t=1}^\infty p_{ii}^{(t)}=\infty \Leftrightarrow  \sum_{t=1}^\infty p_{jj}^{(t)}=\infty.
\end{align*}
%
Da Markov-kæden er ureducerbar, vil $i \leftrightarrow j$ for alle $i,j \in S$. Af \autoref{def:tilgængelighed} gælder det da, at $i \to j$ og $j \to i$, og dermed eksisterer der $t,m\geq 0$ således, at
\begin{align*}
    p_{ij}^{(t)} > 0 \quad \text{ og } \quad p_{ji}^{(m)} > 0.
\end{align*}
Altså
\begin{align*}
    \alpha = p_{ij}^{(t)}p_{ji}^{(m)} > 0.
\end{align*}
Det følger af \autoref{sæt:chapman-kolmogrov}, at
\begin{align*}
    p_{ii}^{(t+r+m)} = \sum_{j \in S} p_{ij}^{(t)} p_{jj}^{(r)}p_{ji}^{(m)} \geq p_{ij}^{(t)} p_{jj}^{(r)}p_{ji}^{(m)} = \alpha p_{jj}^{(r)} \text{ for } r \geq 0.
\end{align*}
Ved at summe over $r$ fås
\begin{align*}
    \sum_{r=0}^\infty p_{ii}^{(t+r+m)} \geq \alpha \sum_{r=0}^\infty p_{jj}^{(r)}.
\end{align*}
Deraf følger det, at hvis $\displaystyle\sum_{r=0}^\infty p_{jj}^{(r)}= \infty$, så er $\displaystyle\sum_{r=0}^\infty p_{ii}^{(t+r+m)} = \infty$. Hvis $\displaystyle\sum_{r=0}^\infty p_{ii}^{(t+r+m)} = \infty$, så følger det analogt, at $\displaystyle\sum_{r=0}^\infty p_{jj}^{(r)}= \infty$. Dermed er \autoref{kor:enten_forbigå_eller_tilbagevend} bevist.
\end{bev}

Det kan herved konkluderes, at hvis én tilstand er tilbagevendende eller forbigående i en ureducerbar Markov-kæde, så er alle tilstande dette.

I følgende eksempel, \autoref{eks:tilbage}, vil det blive vist, hvordan man undersøger, om en Markov-kæde er ureducerbar, og om tilstandene er forbigående eller tilbagevendende.

\begin{eks} \textbf{} \label{eks:tilbage}%Nyt eksempel
\newline
Lad $\bm X$ være en Markov-kæde givet som i \autoref{eks:overgangsmatrice}, hvor overgangsgrafen og overgangsmatricen blev bestemt. Først bestemmes det, om Markov-kæden er ureducerbar. Af \autoref{def:ureducerbar} følger det, at hvis alle tilstande i tilstandsrummet kommunikerer, er Markov-kæden ureducerbar. For at alle tilstande kommunikerer med hinanden, skal der eksistere et $t \in T$ således, at $p_{ij}^{(t)}>0$ for alle $i,j \in S$. Fra \autoref{eks:overgangsmatrice} er det givet, at en del af overgangssandsynlighederne er strengt positive, men da alle overgangssandsynlighederne skal være positive, undersøges overgangsmatricen for andet trin.
\begin{align*}
\mathcal{P}^2&=
\begin{bmatrix}
0 & \frac{1}{2} & \frac{1}{2}\\
0 & \frac{1}{2} & \frac{1}{2}\\
\frac{1}{2} & \frac{1}{2} & 0
\end{bmatrix}\begin{bmatrix}
0 & \frac{1}{2} & \frac{1}{2}\\
0 & \frac{1}{2} & \frac{1}{2}\\
\frac{1}{2} & \frac{1}{2} & 0
\end{bmatrix} =\begin{bmatrix}
\frac{1}{4} & \frac{1}{2} & \frac{1}{4}\\
\frac{1}{4} & \frac{1}{2} & \frac{1}{4}\\
 0 & \frac{1}{2} & \frac{1}{2}\\
\end{bmatrix}.
\intertext{Det gælder stadig ikke, at $p_{ij}^{(t)}>0$ for alle $i,j \in S$. Derfor bestemmes overgangsmatricen for tredje trin.}
\mathcal{P}^3&=
\begin{bmatrix}
\frac{1}{4} & \frac{1}{2} & \frac{1}{4}\\
\frac{1}{4} & \frac{1}{2} & \frac{1}{4}\\
 0 & \frac{1}{2} & \frac{1}{2}\\
\end{bmatrix}\begin{bmatrix}
0 & \frac{1}{2} & \frac{1}{2}\\
0 & \frac{1}{2} & \frac{1}{2}\\
\frac{1}{2} & \frac{1}{2} & 0
\end{bmatrix} =
\begin{bmatrix}
\frac{1}{8} & \frac{1}{2} & \frac{3}{8}\\
\frac{1}{8} & \frac{1}{2} & \frac{3}{8}\\
\frac{1}{4} & \frac{1}{2} & \frac{1}{4}\\
\end{bmatrix}.
\end{align*}
Da $p_{ij}^{(3)}>0$ for alle $i,j \in S$, følger det af \autoref{def:tilgængelighed}, at de tre tilstande er tilgængelige fra hinanden og dermed kommunikerer. Dermed gælder det jævnfør \autoref{def:ureducerbar}, at Markov-kæden er ureducerbar.

Herefter bestemmes det, om tilstandene i Markov-kæden er tilbagevendende eller forbigående. Af \autoref{kor:enten_forbigå_eller_tilbagevend} vides det, at alle tilstande i en ureducerbar kæde enten er forbigående eller tilbagevendende. Derfor undersøges det, om Markov-kæden har én tilbagevendende eller forbigående tilstand. Dette gøres ved at anvende \autoref{tilbagevendende}. Det er i Maple bestemt, hvad $t$'te trins overgangsandsynlighederne er givet ved. Ud fra dette gælder det, at
\begin{align*}
    \sum_{t=1}^\infty p_{22}^{(t)} = \sum_{t=1}^\infty \frac{1}{6} \cdot \left(3 \cdot (0^t +1)\right) = \infty
\end{align*}
Da $\displaystyle\sum_{t=1}^\infty p_{22}^{(t)}=\infty$, er tilstanden $2$ tilbagevendende, hvorved alle tilstandende i Markov-kæden er tilbagevendende. 
Dermed er det blevet bestemt, at Markov-kæden er ureducerbar og alle tilstandene er tilbagevendende.
\end{eks}

Hvis en tilstand i Markov-kæden er tilbagevendende, kan den enten være \textit{positiv-} eller \textit{null tilbagevendende}. Disse begreber defineres som følgende

\begin{defn}\textbf{Positiv og null tilbagevendende}\label{def:positiv_null_tilbagevendende} %Ny definition
\newline
Lad $\bm X$ være en Markov-kæde med tilstandsrummet, $S$, og $i\in S$ være en tilbagevendende tilstand.  Lad derudover  $\tau_i$ være antallet af trin før kæden besøger $i$. Hvis
\begin{enumerate}
    \item $E_i[\tau_i|X_0 = i]<\infty$, siges $i$ at være positiv tilbagevendende.
    \item $E_i[\tau_i|X_0 = i]=\infty$, siges $i$ at være null tilbagevendende.
\end{enumerate}
\end{defn}

Altså gælder det, at hvis $i$ er positiv tilbagevendende, forventes det, at Markov-kæden vender tilbage til tilstanden $i$ indenfor et endeligt antal trin. For en tilstand der er null tilbagevendende, forventes der derimod, at kæden ikke vender tilbage til tilstanden indenfor et endeligt antal trin. % der går uendeligt mange trin, før kæden vender tilbage til tilstanden. 

Ud over at tilstande kan klassificeres som forbigående, positiv tilbagevendende og null tilbagevendende, kan de også opdeles i \textit{aperiodiske} og \textit{periodiske} tilstande. Dette defineres som følgende

\begin{minipage}\textwidth
\begin{defn}\textbf{Perioden for en tilstand}\label{def:aperiodisk} %Ny definition
\newline
Lad $\bm X$ være en Markov-kæde med $t$'te trins overgangssandsynligheden $p_{ij}^{(t)}$, tilstandsrummet $S$ og tilstande $i,j \in S$. Perioden for $i$ er defineret som
\begin{align*}
    d(i) = gcd\{t\geq 1 : p_{ii}^{(t)}>0\}.
\end{align*}
Hvis $d(i)=1$, siges $i$ at være aperiodisk, ellers siges den at være periodisk.
\end{defn}
\end{minipage}

Altså er perioden defineret som den største fælles divisor af antallet af trin for at vende tilbage til $i$. Hvis en Markov-kæde for eksempel vender tilbage til tilstanden $i$ efter $3$ trin, men af en anden vej kan tage $6$ trin, vil den største fælles divisor være $3$. 

Hvis en Markov-kæde er ureducerbar og har én aperiodisk tilstand, gælder følgende lemma. 

%at alle tilstande i Markov-kæden er aperiodiske. Dette er præsenteret i følgende lemma


\begin{minipage}\textwidth
\begin{lem} \label{lem:lort}\textbf{} %Nyt lemma
\newline
Lad $\bm X$ være en ureducerbar Markov-kæde med tilstandsrummet $S$ og tilstande $i, j, k \in S$. Lad derudover $\bm X$ have $t$'te trins overgangssandsynligheden $p_{ij}^{(t)}$. Hvis $\bm X$ har en aperiodisk tilstand $k$, så gælder det, at der for alle tilstande $i$ og $j$, eksisterer et $N\in \N$ således, at $p_{ij}^{(t)}>0$ for alle $t\geq N$. 
\end{lem}
\end{minipage}

\begin{bev} \textbf{} %Nyt bevis
\newline
Lad $\bm X$ være en ureducerbar Markov-kæde med tilstandsrummet $S$ og tilstande $i, j, k \in S$. Lad derudover $\bm X$ have $t$'te trins overgangssandsynligheden $p_{ij}^{(t)}$ og $k$ være en aperiodisk tilstand.

Da $\bm X$ er ureducerbar, følger det af \autoref{def:tilgængelighed} og \autoref{def:ureducerbar}, at der eksisterer et $r, s\geq 0$ således, at overgangssandsynligheder, $p_{ik}^{(r)}, p_{kj}^{(s)}>0$. Da $k$ er aperiodisk, eksisterer der et $N\in \N$ således, at $p_{ij}^{(t)}>0$ for $t\geq N$. Dermed følger det af \autoref{sæt:chapman-kolmogrov}, at
\begin{align*}
    p_{ij}^{(r+t+s)}\geq p_{ik}^{(r)}p_{kk}^{(t)}p_{kj}^{(s)}>0
\end{align*}
Hermed er \autoref{lem:lort} bevist. 
\end{bev}

\section{Invariant fordeling}
Det er muligt at bestemme en Markov-kædes asymptotiske opførsel, når $\P$ er kendt. Dette gøres ved at bestemme $\displaystyle \lim_{t \to \infty} \P^{(t)}$. Hvis overgangssandsynlighederne konvergerer, er en mulig ligevægt for Markov-kæden den invariante fordeling. Den invariante fordeling defineres som følgende
%
\begin{defn}\textbf{Invariant fordeling} \label{defn: invariant_fordeling} %Ny definition
\newline 
Lad $\bm X$ være en Markov-kæde med tilstandsrummet, $S$, og overgangsmatricen, $\P$, lad derudover $i\in S$ være en tilstand. En sandsynlighedsfordeling, $\bm\pi=(\pi_i : i \in S)$, på $S$, som opfylder
\begin{enumerate}
    \item $\pi_i\geq 0 \text{ for } i\in S$,
    \item $\displaystyle\sum_{i\in S} \pi_i=1$,
    \item $\bm\pi \P=\bm\pi$,
\end{enumerate}
kaldes for en \textit{invariant fordeling} af kæden. 
\end{defn}
%Eftersom $S$ er endelig eller tællelig uendelig, er det muligt at definere sandsynlighedsfordelingen som en vektor $\bm {\pi} = (\pi_i: i \in S)$, hvor $\pi_i = p_X(i)$ er en frekvensfunktion over en tilfældig variabel $X$. 
Bemærk, at da $\bm X = (X_0, X_1, \ldots)$ er en Markov-kæde bestående af diskrete tilfældige variabler, er sandsynlighedsrummet endeligt eller tællelig uendeligt. Altså er det muligt at definere sandsynlighedsfordelingen som en vektor $\bm {\pi} = (\pi_i: i \in S)$, hvor $\pi_i = p_{X_t} (i)$ er en frekvensfunktion for en tilfældig variabel $X_t$. 

Den invariante fordeling er invariant i forhold til tiden, altså vil $\bm \pi \P^t = \bm\pi$ for $t = 1, 2, \ldots, N$. 

Derudover opfylder indgangene i $\bm \pi$, at
\begin{align}\label{eq:sum_invariant_fordeling}
     \pi_j=\sum_{i\in S}p_{ij}\pi_i \text{ for alle } j\in S. 
\end{align}

Der eksisterer ikke en invariant fordeling for enhver Markov-kæde, og hvis den eksisterer, er den ikke nødvendigvis entydig. 
Hvis Markov-kæden er ureducerbar, vil den invariante fordeling være entydig såfremt, at den eksisterer. Derudover er der en sammenhæng mellem positiv tilbagevendende tilstande og den invariante fordeling. Disse resultater præsenteres i følgende sætning

\begin{minipage}\textwidth
\begin{thmx} \label{sæt:den_vi_ikke_beviser}\textbf{Invariant fordeling og positiv tilbagevendende tilstande} %Ny sætning
\newline
Lad $\bm X$ være en ureducerbar Markov-kæde med tilstandsrummet, $S$ og tilstande $i, j\in S$. Lad derudover $\tau_i$ være antallet af trin før kæden besøger tilstanden $i$.
\begin{enumerate}
    \item Så eksisterer der en invariant fordeling, $\bm \pi$, hvis og kun hvis, der eksisterer en positiv tilbagevendende tilstand.
    \item Hvis der eksisterer en invariant fordeling $\bm \pi$, så er alle tilstande positiv tilbagevendende og
    \begin{align*}
        \pi_i = \frac{1}{E_i[\tau_i|X_0 = i]} \text{ for alle } i\in S. 
    \end{align*}
Altså er $\bm\pi$ entydig. 
\end{enumerate}
\end{thmx}
\end{minipage}

Denne sætning vil ikke blive bevist i dette projekt, men der henvises til \citep[s. 233-234]{oxford}. 

Af \autoref{sæt:den_vi_ikke_beviser} følger det dermed, at hvis der eksisterer en invariant fordeling for en ureducerbar Markov-kæde, så gælder det, at den er entydig, og alle tilstande er positiv tilbagevendende. Derudover medfører positiv tilbagevendende tilstande, at der eksisterer en invariant fordeling. 

\subsection{Konvergens mod den invariante fordeling}
I dette underafsnit vil resultatet om konvergens mod den invariante fordeling præsenteres. Dette beskriver, hvad en Markov-kæde skal opfylde, for at \textit{grænsefordelingen} er den invariante fordeling. Først defineres grænsefordelingen som følgende

%I dette underafsnit vil hovedresultatet være konvergens mod den invariante fordeling, som beskriver, hvad en Markov-kæde skal opfylde, for at grænsefordelingen er den invariante fordeling. Først defineres grænsefordelingen som følgende

\begin{minipage}\textwidth
\begin{defn}\textbf{Grænsefordeling} %Ny definition
\newline
Lad $\bm X$ være en ureducerbar Markov-kæde med tilstandsrummet, $S$ og tilstande $i, j\in S$. Lad derudover $\bm X$ have $t$'te trins overgangssandsynligheden $p_{ij}^{(t)}$. Såfremt der eksisterer en sandsynlighedsfordeling $\bm q$ på $S$ således, at
\begin{align*}
    p_{ij}^{(t)}\to q_j \text{ for } t\to \infty \text{ for alle } i,j\in S,
\end{align*}
kaldes $\bm q$ for Markov-kædens grænsefordeling. 
\end{defn}
\end{minipage}
%Altså gælder det, at hvis de asymptotiske sandsynligheder ikke afhænger af begyndelsestilstanden, $X_0=i$, kaldes fordelingen på det givne tilstandsrum en \textit{grænsefordeling}.


%Bemærk, at $\bm\pi$ er en rækkevektor med ikke-negative indgange. Sandsynligheden, $\pi_j$, fortæller altså, hvor lang tid der bruges i tilstand j i det lange løb. Derfor er det intuitivt, at der gælder følgende definition.

Følgende resultat præsenterer, hvornår den invariante fordeling er grænsefordelingen.

\begin{minipage}\textwidth
\begin{thmx} \textbf{Konvergens mod den invariante fordeling}\label{sæt:konvergens_for_markov} %Ny sætning
\newline
Lad $\bm X$ være en ureducerbar, aperiodisk og positiv tilbagevendende Markov-kæde med tilstandsrummet, $S$ og tilstande $i, j\in S$. Lad derudover $\bm X$ have $t$'te trins overgangssandsynligheden $p_{ij}^{(t)}$. Så gælder det, at 
\begin{align*}
    p_{ij}^{(t)} \to \pi_j \text{ for } t \to \infty,
\end{align*}
hvor $\bm \pi$ er Markov-kædens entydige invariante fordeling. 
\end{thmx}
\end{minipage}

For at kunne bevise \autoref{sæt:konvergens_for_markov} anvendes nogle hjælperesultater. Disse kan findes i \autoref{hjælpesæt}. 
\vspace{-0.4cm}
\begin{bev} \textbf{} %Nyt bevis
\newline
Lad $\bm X$ være en ureducerbar, aperiodisk og positiv tilbagevendende Markov-kæde med tilstandsrummet, $S$ og tilstande $i, j\in S$. Lad derudover $\bm X$ have $t$'te trins overgangssandsynligheden $p_{ij}^{(t)}$.

Lad $\bm Z = ( \bm X, \bm Y)$ være et ordnet par af uafhængige Markov-kæder, hvor $\bm X = (X_0, X_1, \ldots)$ og $\bm Y = (Y_0, Y_1, \ldots)$. 

Begge Markov-kæder har tilstandsrummet $S$ og overgangsmatrice $\P$. Deraf følger det, at $\bm Z = (Z_0, Z_1, \ldots)$ er en Markov-kæde med tilstandsrum $S \times S$, hvor $Z_t = (X_t, Y_t)$. Af \autoref{def:overgangssandsynlighed} er overgangssandsynlighederne for $\bm Z$ givet ved
\begin{align*}
    p_{ij, kl} &= P\left(Z_{t+1} = (k,l) | Z_t = (i,j)\right).
    \intertext{Da $\bm X$ og $\bm Y$ er uafhængige, følger det af \autoref{def:uafhængighed}, at}
    p_{ij, kl}&=P\left(X_{t+1} = k|X_t = i\right)P\left(Y_{t+1} = l|Y_t = j\right) = p_{ik}p_{jl}.
\end{align*}
Af \autoref{lem:lort} følger det, at for $i,j,k,l \in S$ eksisterer der et $N$ således, at $p_{ik}^{(t)}p_{jl}^{(t)}> 0$ for alle $t \geq N$. Altså eksisterer der ét $t$ således, at $p_{ij, kl}^{(t)}>0$, og $\bm Z$ er dermed ureducerbar som følge af \autoref{def:tilgængelighed} og \autoref{def:ureducerbar}. 

Da $\bm X$ er positiv tilbagevendende, har $\bm X$ en entydig invariant fordeling, $\bm \pi$, som følge af \autoref{sæt:den_vi_ikke_beviser} punkt 1. Dermed må der eksistere en positiv tilbagevendende tilstand i $\bm Z$, og af \autoref{sæt:den_vi_ikke_beviser} punkt 1 gælder det, at $\bm Z$ har en entydig invariante fordeling $\hat{\bm \pi} = (\hat{\pi}_{ij}:i,j\in S)$. Derfor er $\bm Z$ positiv tilbagevendende af \autoref{sæt:den_vi_ikke_beviser} punkt 2. Den invariante fordeling for $\bm Z$ er givet ved produktet mellem $\bm X$ og $\bm Y$'s invariante fordelinger, da $\bm X$ og $\bm Y$ er uafhængige. Derudover har $\bm X$ og $\bm Y$ den samme sandsynlighedsfordeling og dermed også samme invariante fordeling. Den invariante fordeling for $\bm Z$ er altså givet ved $\hat{\pi}_{ij} = \pi_i\pi_j$. %Derfor er $\bm Z$ positiv tilbagevendende af \autoref{sæt:den_vi_ikke_beviser} punkt 2. 

Lad $X_0=i$ og $Y_0=j$ således, at $Z_0=(i,j)$. Lad derudover $s\in S$ være en tilstand sådan at
\begin{align*}
    \tau=\min\{t\geq 1:Z_t=(s,s)\},
\end{align*}
er antallet af trin før kæden besøger $(s,s)$. Altså det mindste antal af trin før Markov-kæderne $\bm X$ og $\bm Y$ er lig hinanden. Da $\bm Z$ er positiv tilbagevendende følger det af \autoref{def:tau}, at
\begin{align}\label{tauss}
    P\left(\tau < \infty |Z_0 = (i, j)\right) = 1.
\end{align}
Altså er $\tau$ endelig. Det skal vises, at grænsefordelingen for $\bm X$ og $\bm Y$ er uafhængige af deres begyndelsesandsynlighed.
\begin{align*}
    p_{ik}^{(t)}&=P\left(X_t=k | Z_0 = (i,j)\right).
    \intertext{Da $\{\tau \leq t\}$ og $\{\tau > t\}$ er disjunkte hændelser, følger det af \autoref{def:sandsynlighedsregningens_grundsætninger} punkt 3, at}
    p_{ik}^{(t)}&=P\left(X_t=k, \tau\leq t | Z_0 = (i,j)\right)+P\left(X_t=k, \tau> t | Z_0 = (i,j)\right).
    \intertext{Af den stærke Markov egenskab (se \autoref{stærk_markov}), med $\tau$ som stoptid (se \autoref{stoptid}), gælder det, at $X_n$ og $Y_n$ har den samme betingede sandsynlighed givet, at $\{\tau \leq n\}$}
    p_{ik}^{(t)}&=P\left(Y_t=k, \tau\leq t| Z_0 = (i,j)\right)+P\left(X_t=k, \tau> t| Z_0 = (i,j)\right)\\
    &\leq P\left(Y_t=k| Z_0 = (i,j)\right)+P\left(\tau> t| Z_0=(i,j)\right)\\
    &= p_{jk}^{(t)} + P\left(\tau> t| Z_0 = (i,j)\right)
\end{align*}
Ud fra ovenstående og \eqref{tauss} fås følgende
\begin{align*}
    \left|p_{ik}^{(t)} - p_{jk}^{(t)}\right| \leq P\left(\tau> t| Z_0 = (i,j)\right) \to 0 \text{ for } t \to \infty
\end{align*}
Dette medfører, at
\begin{align*}
    p_{ik}^{(t)} - p_{jk}^{(t)} \to 0 \text{ for } t \to \infty.
\end{align*}
Hvis grænsen $\displaystyle \lim_{t \to \infty} p_{ik}^{(t)}$ eksisterer, afhænger den dermed ikke af $i$. 

Herefter vises det, at grænsen eksisterer. Fra \eqref{eq:sum_invariant_fordeling} og \autoref{defn: invariant_fordeling} punkt 2 følger det, at
\begin{align*}
    \pi_k-p_{jk}^{(t)}&=%\sum_{i\in S}\pi_ip_{ik}^{(t)}-p_{jk}^{(t)}\\
    \sum_{i\in S} p_{ik}^{(t)} \pi_i - \sum_{i\in S} \pi_i p_{jk}^{(t)}=\sum_{i\in S}\pi_i\left(p_{ik}^{(t)}-p_{jk}^{(t)}\right) 
\end{align*}
Da $ p_{ik}^{(t)} - p_{jk}^{(t)} \to 0$ for $t\to \infty$, følger det af \autoref{sidstelemma}, at $\displaystyle\sum_{i\in S}\pi_i\left(p_{ik}^{(t)}-p_{jk}^{(t)}\right)\to 0 $ for $ t \to \infty$.
Altså gælder det, at $p_{ij}^{(t)} \to \pi_j \text{ for } t \to \infty$, og dermed er \autoref{sæt:konvergens_for_markov} bevist.
\end{bev}

Det vil i følgende eksempel, \autoref{eks:invariant}, blive vist, hvordan man undersøger, om en Markov-kæde har en invariant fordeling. Derudover vil den invariante fordeling blive bestemt, og til sidst vil det konkluderes, om Markov-kæden har den invariante fordeling som grænsefordeling.

\begin{eks}\textbf{}\label{eks:invariant}\\
Lad $\bm X = (X_0, X_1, \ldots)$ være en Markov-kæde givet som i \autoref{eks:overgangsmatrice}. Det vil først bestemmes, om denne Markov-kæde har en invariant fordeling. Af \autoref{sæt:den_vi_ikke_beviser} gælder det, at hvis Markov-kæden har en positiv tilbagevendende tilstand, så er alle tilstandene positiv tilbagevendende, og der eksisterer en entydig invariant fordeling. Fra \autoref{eks:tilbage} er det bestemt, at Markov-kæden er ureducerbar, og at alle tilstandene er tilbagevendende. Derfor undersøges det, om en af tilstandene er positiv tilbagevendende. Dette gøres ud fra \autoref{def:positiv_null_tilbagevendende}.
\begin{align*}
    E_2[\tau_2| X_0 = 2]&=\sum_{k\in \N} kP(\tau_2=k|X_0=2)\\
    &=\frac{1}{2} + \left(\frac{1}{2}\right)^2 + \left(\frac{1}{2}\right)^3 + \left(\frac{1}{2}\right)^4 + \ldots\\
    &= \sum_{k \in \N} \left(\frac{1}{2}\right)^k = 1
\end{align*}
Da $ E_2[\tau_2 | X_0 = 2]<\infty$, følger det af \autoref{def:positiv_null_tilbagevendende}, at tilstanden, $2$, er positiv tilbagevendende, hvorved alle tilstandene i Markov-kæden er positiv tilbagevendende, og derfor eksisterer der en entydig invariant fordeling.

Ud fra \autoref{defn: invariant_fordeling} skal den invariante fordeling opfylde, at 
\begin{align*}
    \begin{bmatrix}
    \pi_1 & \pi_2 & \pi_3
    \end{bmatrix}
    \begin{bmatrix}
    0 & \frac{1}{2} & \frac{1}{2}\\
    0 & \frac{1}{2} & \frac{1}{2}\\
    \frac{1}{2} & \frac{1}{2} & 0
    \end{bmatrix} &= \begin{bmatrix}
    \pi_1 & \pi_2 & \pi_3
    \end{bmatrix}.
    \intertext{Dette opstilles som tre ligninger med tre ubekendte.}
    \frac{1}{2}\pi_3 &=\pi_1,\\
    \frac{1}{2}\pi_1 + \frac{1}{2} \pi_2+ \frac{1}{2}\pi_3 &=\pi_2, \\
    \frac{1}{2}\pi_1 + \frac{1}{2} \pi_2 &= \pi_3.
    \intertext{Dermed skal det gælde, at}
    \pi_3 = 2\pi_1 \text{ og } \pi_2 &= 3\pi_1
\end{align*}
Af \autoref{defn: invariant_fordeling} skal det være opfyldt, at $\displaystyle \sum_{i \in S} \pi_i = 1$, og dermed er den invariante fordeling givet ved følgende
\begin{align*}
    \bm \pi = \begin{bmatrix}
    \frac{1}{6} & \frac{1}{2} & \frac{1}{3}
    \end{bmatrix}.
\end{align*}

Til sidst vil det blive bestemt, om Markov-kæden har den invariante fordeling som grænsefordeling. For at undersøge om Markov-kæden konvergerer mod den invariante fordeling, skal det jævnfør \autoref{sæt:konvergens_for_markov} også gælde, at den er aperiodisk. Da der for $t=3$ gælder, at $p_{ij}^{(t)} > 0$ for alle $i,j \in S$, er alle tilstande i tilstandsrummet aperiodiske. Da Markov-kæden er ureducerbar, aperiodisk og positiv tilbagevendende, følger det af \autoref{sæt:konvergens_for_markov}, at den konvergerer mod den invariante fordeling. 
\end{eks}



