For at introducere begrebet, \textit{forventet værdi}, bliver der gjort brug af \textit{gennemsnitlige værdier} af et eksperiment. De gennemsnitlige værdier er data, som kan give et billede af, hvordan eksperimentet i gennemsnit vil udfalde sig.
Mere præcist fremgår det i følgende definition.

\begin{minipage}\textwidth
\begin{defn}\textbf{Forventet værdi} \label{def:Forventetværdi} %Ny definition 
\newline
Lad X være en diskret tilfældig variabel med range $\{x_1,x_2,\ldots\}$ og  \textit{p} være pmf. Den forventede værdi af X er defineret ved
\begin{align}
E[X]=\sum_{k=1}^\infty x_k p(x_k)
\end{align}
\end{defn}
\end{minipage}



Den forventede værdi er altså et vægtet gennemsnit af de mulige værdier af X med tilsvarende sandsynligheder som vægte. Det kan tænkes som værende et teoretisk rumlig gennemsnit over X's range, hvoraf der i det ovenstående er beskrevet, at det fortløbende gennemsnit er tidsgennemsnittet. Desuden fremgår, at den forventede værdi er et tal, der er beregnet ud fra fordelingen, og tidsgennemsnittet er beregnet fra eksperimentet og er dermed tilfældigt.
\\
\\
I nogle tilfælde vil den tilfældige variabel være ikke-negativ, derved gælder følgende.


\begin{minipage}\textwidth
\begin{thmx} \textbf{} %Ny sætning
\newline
Lad X være en diskret tilfældig variabel med range $\{0,1,\ldots\}$, så gælder
\begin{align*}
    E[X]=\sum_{n=0}^\infty P(X > n)
\end{align*}
\end{thmx}
\end{minipage}


\begin{bev} \textbf{}
Lad $K=\sum_{n=1}^k 1$\\ Ved brug af definition \ref{Definition2.4.1} fås
\begin{align*}
    E[X]=\sum_{k=1}^\infty kP(X=k) = \sum_{k=1}^\infty \sum_{n=1}^k P(X=k)\\
    = \sum_{k=1}^\infty \sum_{n=1}^\infty P(X=k) = \sum_{n=1}^\infty P(X \geq n)\\
    = \sum_{n=0}^\infty P(X > n)\\
    \end{align*}
\end{bev}


Den forventede værdi kan opstilles som en funktion, heraf ses følgende

\begin{minipage}\textwidth
\begin{pro} \textbf{Forventet værdi som funktion af diskret variabel} %Ny proposition
\newline
Lad X være en vilkårlig diskret variabel med $pmf\  p_X$, og $range\{x_1,x_2,\cdots\}$. Lad desuden $g: R \to R$ være en vilkårlig funktion. Så gælder det, at
\begin{align}
    E[g(X)]=\sum_{k=1}^\infty g(x_k)p_X(x_k)
\end{align}
\end{pro}
\end{minipage}

\begin{bev}\textbf{} %Nyt bevis
\newline
Lad $I = range\{x_1, x_2, \cdots\}$ og lad $Y = g(I)$. Så gælder

\begin{align*}
    E(Y)&=\sum_{k=1}^\infty y_k p_Y(y_k)\\
    &=\sum_{k=1}^\infty y_k p_X(x_k)\\
    &=\sum_{k=1}^\infty g(x_k)p_X(x_k)
\end{align*}
%\begin{align*}
%    E(Y)&=\sum_{y \in g(I)} y p_Y(y)\\
%    &=\sum_{y\in g(I)} y \sum_{x\in I:g(x)=y} p_X(x)\\
%    &=\sum_{x\in I} g(x)p_X(x)
%\end{align*}
%hvis den sidste konvergerer absolut
\end{bev}


\section{Varians} %fra s. 104 osv.
Det er tidligere nævnt, at den forventede værdi beskriver den samlede fordeling af et tilfældigt tal. Dette giver således et billede af $X$'s gennemsnitlige værdi, dertil indføres begrebet \textit{lokationsparameteret}. Den forventede værdi giver dog ikke information omkring $X's$ variabilitet. 
Der tages udgangspunkt i følgende eksempel.
\begin{eks}\textbf{}
\newline
Lad $X \sim  unif [w-0.01, w+0.01]$ og $Y  \sim unif[w-0.1, w+0.1]$ beskrive variabiliteten for et objekt. De forventede værdi vil i dette omfang give
$$E[X] = w \ og \ E[Y] = w$$
begge beskriver gennemsnittet, men det er klart, at $E[X]$ varierer tættere på gennemsnittet end $E[Y]$. Den forventede værdi giver altså ikke et billede af, hvor meget variation der vil være.\\
\end{eks}

Ovenstående eksempel er motivation til begrebet, \textit{varians}. Varians beskriver således den variation, som X ligger i. Lad nu $\mu$ beskrive den forventede værdi for lokationen. Dertil vil gennemsnittet beskrives ved hjælp af $X - \mu$, hvor $|X-\mu|$ beskriver den generelle afvigelse fra gennemsnitsværdien, for et vilkårligt $\mu$.
Dertil fremgår det af følgende definition

\begin{minipage}\textwidth
\begin{defn}\textbf{Varians}\label{def:varians} %Ny definition
\newline
Lad X være en tilfældig variabel med forventet værdi $E[x]$. \textit{Variansen} af X er defineret ved
\begin{align*}
    Var[X]=E[(X-E[x])^2] \geq 0
\end{align*}
\end{defn}
\end{minipage}

Fremadrettet vil varians beskrives ved $\sigma^2$. Bemærk desuden at varians vil være større eller lig den gennemsnitlige værdi, hvor der i tilfældet med en højere varians vil medføre en større afvigelse.


\begin{minipage}\textwidth
\begin{defn}\textbf{Standardafvigelse} %Ny definition
\newline
Lad X være en tilfældig variabel med varians $\sigma^2 = Var[X]$. Standardafvigelsen for X er defineret ved $\sigma = \sqrt{Var[X]}$
\end{defn}
\end{minipage}


Der gælder desuden følgende korollar 

\begin{minipage}\textwidth
\begin{kor} \textbf{} %Nyt korollar
\newline
\begin{align*}
    Var[X]=E[X^2] - (E[X])^2
\end{align*}
\end{kor}
\end{minipage}

\begin{bev} \textbf{} %Nyt bevis
\newline
Der bliver gjort brug af \autoref{def:Forventetværdi}, hvor\\
$$E[X] = \sum_{k=1}^\infty x_k p(x_k)$$.\\
Der gælder desuden jævnført \autoref{def:varians}, at\\
$$Var[X] = E[(X-E[X])^2]$$
for diskrete variable gælder fra \textbf{prob 2.12}
\begin{align*}
    Var[X] &=\sum_{k=1}^\infty(x_k-E[X])^2 p(x_k)\\
    &= \sum_{k=1}^\infty (x_k^2-2x_kE[X]+(E[X])^2)p(x_k)\\
    &= \sum_{k=1}^\infty x_k^2p(x_k) - 2E[X] \sum_{k=1}^\infty x_k p(x_k) + (E[X])^2 \sum_{k=1}^\infty p(x_k)\\
    &= E[X^2]-2(E[X])^2+(E[X])^2
    = E[X^2] - (E[X])^2
\end{align*}
\end{bev}


\begin{eks} \textbf{Varians for terningeslag} %Nyt eksempel
\newline
Lad X være et tilfældigt tal på en terning.\\
Find $Var[X]$\\
Pmf er givet ved $p(k) = \dfrac{1}{6}$, $k=1,2,\ldots, 6$, dette giver den forventede værdi\\
\begin{align*}
    E[X]=\sum_{k=1}^6 kp(k) = \dfrac{1}{6}\sum_{k=1}^6 k=\dfrac{7}{2}\\
\end{align*}
Ved brug af sætning ... fås\\
    
\begin{align*}
    E[X^2]=\sum_{k=1}^6 k^2 p(k) = \dfrac{1}{6} \sum_{k=1}^6 k^2 = \dfrac{91}{6}
\end{align*}
Dertil bruges definitionen af varians\\
\begin{align*}
    Var[X]=E[X^2]-(E[X])^2=\dfrac{35}{12}
\end{align*}
\end{eks}











\iffalse
Der kan ydermere defineres den forventede værdi som værende kontinuert. Hertil bliver summen erstattet af integraler, nærmere beskrevet.

\begin{minipage}\textwidth \label{Definition 2.9}
\begin{defn}\textbf{} %Ny definition
\newline
Lad X være en kontinuert tilfældig variabel med pdf \textit{f}. Den forventede værdi af X er defineret ved\\
\begin{align*}
    E[X]=\int_{-\infty}^\infty xf(x)dx
\end{align*}
\end{defn}
\end{minipage}

De formelle grænseværdier er givet ved $-\infty$ og $\infty$, men i realiteten er grænserne givet ved $X$'s range, som er de  værdier, hvor $f(x)$ er positiv.\\

\begin{eks} Lad $X \sim unif[a,b]$. Bestem $E[X]$.\\
Af definition 2.9 fremgår,\\
\begin{align*}
    E[X]=\int_{-\infty}^\infty xf(x)dx = \dfrac{1}{b-a}\int_{a}^b xdx = \dfrac{a+b}{2}
\end{align*}
hvilket angiver midtpunktet af intervallet $[a,b]$.
\end{eks}
\\
For ikke-negative kontinuerte tilfældige variable gælder analogt med definition \ref{Definition 2.9}.
\begin{thmx}
Lad X være en kontinuert tilfældig variabel med range $[0,\infty].$ Derved gælder
\begin{align*}
    E[X]=\int_{0}^\infty P(X > x) dx
\end{align*}
\end{thmx}
\begin{bev}
\textbf{Mangler at bevise denne sætning.}
\end{bev}

\begin{eks}
Evt. et eksempel for brugen af ovenstående sætning.
\end{eks}

\textbf{Det er også muligt at komme ind på, at den foreventede værdi er lineær, men det ved jeg ikke, om det er relevant.}
\fi

